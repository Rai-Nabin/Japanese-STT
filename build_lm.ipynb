{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract sentences from csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data/sentence_jpn.txt', mode='a', encoding='utf-8') as text_file:\n",
    "    with open('data/cv-corpus-10/clips/test.csv', mode='r') as csv_file:\n",
    "        reader = csv.reader(csv_file)\n",
    "\n",
    "        for idx, row in enumerate(reader):\n",
    "            if idx != 0:\n",
    "                sentence = row[2]\n",
    "                text_file.write(sentence)\n",
    "                text_file.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the guide: https://kheafield.com/code/kenlm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting to lowercase and counting word occurrences ...\n",
      "| |#                                               | 4394 Elapsed Time: 0:00:00\n",
      "\n",
      "Saving top 4000 words ...\n",
      "\n",
      "Calculating word statistics ...\n",
      "  Your text file has 4395 words in total\n",
      "  It has 4394 unique words\n",
      "  Your top-4000 words are 91.0353 percent of all words\n",
      "  Your most common word \"ちょっと待ってください\" occurred 2 times\n",
      "  The least common word in your top-k is \"シャンチの専業プロはチムから支払われる給料と対局費を主な収入としている\" with 1 times\n",
      "  The first word with 2 occurrences is \"ちょっと待ってください\" at place 0\n",
      "\n",
      "Creating ARPA file ...\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/nabin/office-work/Japanese-STT/scorer/lower.txt.gz\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 4395 types 4397\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:52764 2:1374936064 3:2578005248 4:4124808192 5:6015345664\n",
      "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 3: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 4: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 4397 D1=0.5 D2=1 D3+=1.5\n",
      "2 8787 D1=0.5 D2=1 D3+=1.5\n",
      "3 1/4393 D1=0.5 D2=1 D3+=1.5\n",
      "4 0 D1=0.5 D2=1 D3+=1.5\n",
      "5 0 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 317 assuming -p 1.5\n",
      "probing 386 assuming -r models -p 1.5\n",
      "trie    185 without quantization\n",
      "trie    142 assuming -q 8 -b 8 quantization \n",
      "trie    184 assuming -a 22 array pointer compression\n",
      "trie    141 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:52764 2:140592 3:20 4:24 5:28\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:52764 2:140592 3:20 4:24 5:28\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "\n",
      "Name:lmplz\tVmPeak:13935212 kB\tVmRSS:6744 kB\tRSSMax:2442788 kB\tuser:0.339145\tsys:0.815563\tCPU:1.15474\treal:1.12147\n",
      "\n",
      "Filtering ARPA file using vocabulary of top-k words ...\n",
      "Reading scorer/lm.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "\n",
      "Building lm.binary ...\n",
      "Reading scorer/lm_filtered.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Identifying n-grams omitted by SRI\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Quantizing\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Writing trie\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!python STT/data/lm/generate_lm.py \\\n",
    "    --input_txt data/sentence_jpn.txt \\\n",
    "    --output_dir scorer/ \\\n",
    "    --top_k 4000 \\\n",
    "    --kenlm_bins kenlm/build/bin/ \\\n",
    "    --arpa_order 5 \\\n",
    "    --max_arpa_memory \"85%\" \\\n",
    "    --arpa_prune \"0|0|1\" \\\n",
    "    --binary_a_bits 255 \\\n",
    "    --binary_q_bits 8 \\\n",
    "    --binary_type trie \\\n",
    "    --discount_fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package the Language Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 unique words read from vocabulary file.\n",
      "Doesn't look like a character based (Bytes Are All You Need) model.\n",
      "--force_bytes_output_mode was not specified, using value infered from vocabulary contents: false\n",
      "Package created in scorer/kenlm.scorer.\n"
     ]
    }
   ],
   "source": [
    "!./native_client/generate_scorer_package \\\n",
    "    --checkpoint checkpoints/japanese_chekpoints \\\n",
    "    --lm scorer/lm.binary \\\n",
    "    --vocab scorer/vocab-4000.txt \\\n",
    "    --package scorer/kenlm.scorer \\\n",
    "    --default_alpha 0.931289039105002 \\\n",
    "    --default_beta 1.1834137581510284"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b324ffdcc398f29b17288a8fd58be02b680f16d9101554ab09bdfe77c93f447b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
